# ğŸ­ Tommie Mouse Reinforcement Learning Maze Project
University of St. Thomas â€“ SEIS 764 Final Project
This project explores how reinforcement learning (RL), classical pathfinding, and deep learning solve grid-based mazes. It includes:
A fully custom Gymnasium-compatible maze environment
Tabular Q-learning on fixed and randomly generated mazes
Evaluation on a large dataset (perfect + imperfect mazes)
A fast A* shortest-path benchmark
Optional emoji visualization
Planned upgrade to Deep Q-Networks (DQN)

# ğŸ”§ 1. Activate Python Environment
source rl_env/bin/activate
Make sure required packages (Gymnasium, NumPy, Matplotlib, Pygame, Pillow) are installed.

# ğŸ§± 2. Maze Environment (maze_env.py)
The environment defines:
Grid maze with 0 = free and 1 = wall
Agent start: (0, 0)
Goal: bottom-right cell
Actions: up, right, down, left
Optional emoji visualization (ğŸ­ + ğŸ§€)
Compatible with any N Ã— N maze size
Test the environment
python test_maze_env.py
This will run random actions to verify:
Maze loads correctly
Walls block movement
Rendering works correctly
Gymnasium documentation: https://gymnasium.farama.org

# ğŸ§  3. Tabular Q-Learning (Fixed Maze)
File: q_learning_fixed_maze.py
This script:
Loads one maze
Uses tabular Q-learning
Learns state-action values
Uses reward shaping:
+20 when reaching the goal
-5 for hitting a wall
-1 per move
Great for learning the fundamentals and validating RL logic.

# ğŸ” 4. Tabular Q-Learning on Random Generated Mazes
File: q_learning_gen_maze.py
This version:
Uses maze_generator.py
Generates solvable mazes at increasing difficulty
Lets the agent learn general patterns instead of memorizing a layout
Limitations:
Tabular Q-learning cannot scale to larger mazes
Works well only on ~5Ã—5 or similar sizes

# ğŸ“ 5. Maze Dataset (Perfect + Imperfect Mazes)
Located in:
mazes/
    perfect_maze/
    imperfect_maze/
The dataset contains 3,000 total mazes at random sizes (10Ã—10 â†’ 150Ã—150+).
Scripts:
maze_dataset.py â€” loads maze files, handles indexing, lazy loading
maze_decoder.py â€” converts irregular raw text formats into numpy grids
This is required to train RL models on real, pre-generated maze distributions.

# â­ 6. A* Shortest-Path Benchmark
File: train_shortest_path.py
Runs optimal A* pathfinding on the dataset.
Features:
Works on all maze sizes
Finds optimal shortest path
Detects unsolvable mazes
Optional visualization
Helps benchmark maze difficulty before using RL
You can control rendering:
RENDER = True
RENDER_FIRST_K = 3
This shows only the first K solved mazes â€” avoids dozens of Pygame windows.

# ğŸ¨ 7. Visualization Improvements
Rendering includes:
Purple walls
White free-space
ğŸ­ mouse for the agent
ğŸ§€ cheese for the goal
Smooth animation when stepping through a solution path
Pygame rendering can be turned off during training and on during evaluation.

# ğŸš€ 8. Future Work: Deep Q-Network (DQN)
Because tabular Q-learning cannot generalize to large mazes (100Ã—100+), the next stage is:
Build a Deep Q-Network (DQN):
CNN-based state encoder
Replay buffer
Target network
Îµ-greedy policy
Mini-batch training
The DQN pipeline will:
Convert maze grids to tensor inputs
Learn generalized navigation strategies
Scale to large mazes
Train across the entire dataset
This is the natural continuation of the project into modern RL.

# âœ” Repository Structure Summary
Component	Description
maze_env.py	Custom Gymnasium Maze Environment
test_maze_env.py	Test harness for visualization
maze_generator.py	Creates solvable random mazes
q_learning_fixed_maze.py	Tabular RL on one maze
q_learning_gen_maze.py	Tabular RL with curriculum
maze_dataset.py	Loads dataset mazes
maze_decoder.py	Converts text mazes â†’ arrays
train_shortest_path.py	A* solver & benchmark
mazes/	Perfect + imperfect dataset
